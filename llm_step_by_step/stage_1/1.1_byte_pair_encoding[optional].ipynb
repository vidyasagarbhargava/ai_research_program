{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Byte pair encoding from scratch [Andrej Karpathy video]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Good tokenization webapp : https://tiktokenizer.vercel.app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: -**What is string in python?**\n",
    "\n",
    "A:- A string are immutable sequences of Unicode code points.  \n",
    "\n",
    "\n",
    "So what are unicode code points?\n",
    "\n",
    "It is text encoding standard maintained by Unicode Consortium designed to support the use of text written in all world's major writing systems.  \n",
    " It supports 148813 characters and 161 scripts\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unicode code point access by ord function in python\n",
    "\n",
    "ord(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128075"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"ðŸ‘‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8364"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"â‚¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 128075,\n",
       " 44,\n",
       " 32,\n",
       " 104,\n",
       " 111,\n",
       " 119,\n",
       " 32,\n",
       " 97,\n",
       " 114,\n",
       " 101,\n",
       " 32,\n",
       " 121,\n",
       " 111,\n",
       " 117,\n",
       " 32,\n",
       " 100,\n",
       " 111,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 63]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"hello ðŸ‘‹, how are you doing?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q:- **why can't we simply use unicode code points and not tokenization ?**\n",
    "\n",
    "1. It can vocabulary quite long of 148813 code points.\n",
    "2. It is very much alive and not stable \n",
    "\n",
    "We need something better and here comes encodings and there are 3 types of encodings\n",
    "\n",
    "1. UTF-8\n",
    "2. UTF-16\n",
    "3. UTF-32\n",
    "\n",
    "\n",
    "These encodings takes unicode text and translate into binary data or byte streams.\n",
    "\n",
    "> UTF-8 is the most common\n",
    "\n",
    "\n",
    "Disadvantage of using UTF-8 is it imply vocab length of only 256 tokens.Also it makes all our text stretch out very long sequence of bytes. So it is very inefficient for next token prediction task.\n",
    "\n",
    "\n",
    "So we don't want to use raw byte of UTF-8, we want to support larger vocab size but stick to UTF-8 encodings of these strings. So answer is Byte Pair encodings which allows us to compress these byte sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 145,\n",
       " 139,\n",
       " 44,\n",
       " 32,\n",
       " 104,\n",
       " 111,\n",
       " 119,\n",
       " 32,\n",
       " 97,\n",
       " 114,\n",
       " 101,\n",
       " 32,\n",
       " 121,\n",
       " 111,\n",
       " 117,\n",
       " 32,\n",
       " 100,\n",
       " 111,\n",
       " 105,\n",
       " 110,\n",
       " 103,\n",
       " 63]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"hello ðŸ‘‹, how are you doing?\".encode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
